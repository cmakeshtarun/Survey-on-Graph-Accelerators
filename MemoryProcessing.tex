\section{In-Memory Processing}
The In-memory processing techniques focus on bridging the bandwidth gap between compute resources and memory elements by moving them close to each other. With new memory technologies, it is now possible to embed logical operations on the memory cell itself. These techniques have merits and demerits of their own which needs to be properly studied before picking one for an application.

  \subsection{Near-Data Processing}
  The Near-Data Processing (NDP) architectures\cite{Biscuit, Graphicionado} focuses on bringing the data close to the memory by employing various techniques. They replace the conventional hardware-managed cache hierarchy with explicitly managed memory hierarchy. The management technique can be suited to application and system needs. Also these frameworks usually consider the memory hierarchy all the way till secondary storage, involving Disks and SSDs, to improve performance. However, it is possible to improve performance by extracting more bandwidth, when the processing is integrated within memory avoiding the off-chip communication.
  
  \subsection{Processing In-memory}
  Advances in 3D integration on memory devices gave rise opportunities to integrate logic into the memory cells. Some of the integration techniques with various memory technologies are discussed below.
  
    \subsubsection{SRAM-PIM}
      SRAM-PIM is about integrating basic bit-operations logic with the SRAM cells. The paper \cite{prsram} presents a SRAM-PIM based pattern recognition accelerator, which exploits the relaxed precision and linearity requirements of pattern recognition applications to utilise the SRAM based compute memory. The work in \cite{cam} introduces a new content addressable memory (CAM) based on the conventional 6T SRAM cells with configurable memory and logic functions. The processing involved in a search operation in CAM is offloaded to the logic in the memory improving system performance and efficiency. The paper on Compute Caches \cite{computecache} uses emerging bit-line SRAM circuit technology to re-purpose existing cache elements and transforms them into active very large vector computational units. In-place Compute Cache reduces data movement overhead between a cache's sub-arrays and its controller while restricting the type of operations that can be supported and, the placement of operands. The work \cite{sramml} is a machine learning accelerator based on SRAM PIM however with the overhead of large DACs. The paper \cite{6Tsram} presents a novel 6T SRAM cell using the n-well as the write wordline to perform write operations and eliminate the write access transistors, achieving 15\% area saving compared to conventional 8T SRAM.

    \subsubsection{DRAM-PIM}
      DRAM-PIM technology builds on the practically demonstrated 3D stacking of silicon dies. This technology not only reduces the communication latency across banks but also offers provisions to embed simple logic layers in the memory die. The Hybrid Memory Cube (HMC) \cite{hmc, amc} supports upto 8 layers of DRAM dies and an additional logic layer integrated by Through-Silicon Vias (TSV). HMC provides more than 10x increase in bandwidth with less than 1/3rd the energy consumed by a commercial DDR4 module. The embedded logic layer can be equipped with basic operations to pre-process the data before providing it to the host processor for further processing. This reduces the amount of data movement between the processor and the memory leading to efficient usage of limited memory bandwidth and lowered energy consumption.
      
      The performance of HMC is limited by the bandwidth between the memory banks and logic layer. Another technique to improve the DRAM performance is by integrating logic with the memory cells and sense amplifier. The paper \cite{bulkbit} models the bitwise AND and OR operations on data across two DRAM rows, within the memory banks. This techniques achieves over 9X throughput and 50x reduced energy consumption for bulk bitwise operations. However, the read operations on DRAM cells are destructive and needs to be backed up before invoking any operations on them. The DRISA paper \cite{drisa} presents modification at the cell (1-Transistor and 1-Capacitor) and sense amplifier level to optimise the above overheads. The architecture presented in the In-Memory Intelligence(IMI) paper \cite{imi} uses simple bit-serial computing elements attached to the DRAM array's sense amplifier. The IMI architecture is a standard DRAM in form and function with the ability for massive SIMD parallelism and a standard and familiar programming model.

    \subsubsection{NVM-PIM}
      The charge-based memory technology doesn't scale as much as the CMOS technology and has lead to a gap in performance between the memory and logic. The recently developed resistive memory technology \cite{memres} shows promising capabilities in bridging the gap between the storage and processor. Resistive memories including Phase change memories and Spin torque memories change the resistance of the storage elements and can scale better than the charge-based memories. However, as the resistive memories need to change the phase of the element, it incurs high write energy and long write latency. In spite of the above limitation, it offers non-volatility, high density and low overall power-consumption compared to the CMOS technology\cite{tcamrram}.
      
      One class of PIM technique \cite{xbarcomp, xbrml} use the memresistive devices connected in a crossbar structure with peripheral circuitry to realise various logic functions. The other class of PIM technique \cite{magic,imply} exploit analog properties of memresistive devices to realise logic on the memory cell itself. The work Memresistor-Aided Logic (MAGIC) \cite{magic} realises the NOR and other basic operation on a memory cell while retaining the input data. The idea in MAGIC is further extended in \cite{magic-add} to support bitwise addition and subtraction using memresistive crossbar. This marks of the state of the art in NVM-based Processing-In-Memory technology.